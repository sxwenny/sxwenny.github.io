<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/img/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/img/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/img/avatar.jpg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sxwenny.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="记录模块torch.nn.Parameter、torch.nn.Module、torch.autograd.Function，和常见的函数。">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch相关">
<meta property="og:url" content="https://sxwenny.github.io/2020/11/09/PyTorch%E7%9B%B8%E5%85%B3/index.html">
<meta property="og:site_name" content="Xiaowen&#39;s Blog">
<meta property="og:description" content="记录模块torch.nn.Parameter、torch.nn.Module、torch.autograd.Function，和常见的函数。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://sxwenny.github.io/download/20.png">
<meta property="og:image" content="https://sxwenny.github.io/download/21.png">
<meta property="og:image" content="https://sxwenny.github.io/download/25.png">
<meta property="og:image" content="https://sxwenny.github.io/download/26.png">
<meta property="og:image" content="https://sxwenny.github.io/download/22.png">
<meta property="og:image" content="https://sxwenny.github.io/download/23.png">
<meta property="og:image" content="https://sxwenny.github.io/download/24.png">
<meta property="article:published_time" content="2020-11-09T13:51:22.000Z">
<meta property="article:modified_time" content="2021-01-02T08:45:02.000Z">
<meta property="article:author" content="Xiaowen">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://sxwenny.github.io/download/20.png">

<link rel="canonical" href="https://sxwenny.github.io/2020/11/09/PyTorch%E7%9B%B8%E5%85%B3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch相关 | Xiaowen's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xiaowen's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/sxwenny" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sxwenny.github.io/2020/11/09/PyTorch%E7%9B%B8%E5%85%B3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/img/avatar.jpg">
      <meta itemprop="name" content="Xiaowen">
      <meta itemprop="description" content="Learn something">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xiaowen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch相关
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-11-09 21:51:22" itemprop="dateCreated datePublished" datetime="2020-11-09T21:51:22+08:00">2020-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-02 16:45:02" itemprop="dateModified" datetime="2021-01-02T16:45:02+08:00">2021-01-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>记录模块torch.nn.Parameter、torch.nn.Module、torch.autograd.Function，和常见的函数。</p>
<span id="more"></span>
<h2 id="torch-nn-Parameter和torch-nn-Module"><a href="#torch-nn-Parameter和torch-nn-Module" class="headerlink" title="torch.nn.Parameter和torch.nn.Module"></a>torch.nn.Parameter和torch.nn.Module</h2><h3 id="Parameter和Module的官方说明"><a href="#Parameter和Module的官方说明" class="headerlink" title="Parameter和Module的官方说明"></a>Parameter和Module的官方说明</h3><p><strong>torch.nn.Parameter:</strong><br>Arguments: data&#x3D;None, requires_grad&#x3D;True.</p>
<p><strong>torch.nn.Module:</strong><br><code>double()</code> <code>float()</code> <code>half()</code> Casts all floating point parameters and buffers to double&#x2F;float&#x2F;half datatype.<br><code>type(dst_type)</code> Casts all parameters and buffers to dst_type.</p>
<p><code>cpu()</code> <code>cuda(device=None)</code> Moves all model parameters and buffers to the CPU&#x2F;GPU.</p>
<p><code>eval()</code> <code>train(mode=True)</code> Sets the module in evaluation&#x2F;training mode.</p>
<p><code>add_module(name, module)</code> Adds a child module to the current module. The module can be accessed as an attribute using the given name.</p>
<p><code>apply(fn)</code> Applies fn recursively to every submodule (as returned by .children()) as well as self.</p>
<p><code>extra_repr()</code> Set the extra representation of the module. To print customized extra information, you should reimplement this method in your own modules. Both single-line and multi-line strings are acceptable.</p>
<p><code>forward(*input)</code> Defines the computation performed at every call.</p>
<p><code>register_backward_hook(hook)</code> Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed.<br><code>register_forward_hook(hook)</code> Registers a forward hook on the module. The hook will be called every time after <code>forward()</code> has computed an output.<br><code>register_forward_pre_hook(hook)</code> Registers a forward pre-hook on the module. The hook will be called every time before <code>forward()</code> is invoked.</p>
<p><code>requires_grad_(requires_grad=True)</code> Change if autograd should record operations on parameters in this module. This method sets the parameters’ requires_grad attributes in-place.</p>
<p><code>to(*args, **kwargs)</code> Moves and&#x2F;or casts the parameters and buffers.</p>
<p><code>zero_grad()</code> Sets gradients of all model parameters to zero.</p>
<p><code>buffers(recurse=True)</code> Returns an <strong>iterator</strong> over module buffers.<br><code>parameters(recurse=True)</code> Returns an <strong>iterator</strong> over module parameters.<br><code>named_buffers(prefix=&#39;&#39;, recurse=True)</code> Returns an <strong>iterator</strong> over module buffers, yielding both the name of the buffer as well as the buffer itself.<br><code>named_parameters(prefix=&#39;&#39;, recurse=True)</code> Returns an <strong>iterator</strong> over module parameters, yielding both the name of the parameter as well as the parameter itself.<br><code>register_buffer(name, tensor)</code> Adds a persistent buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. Buffers can be accessed as attributes using given names.<br><code>register_parameter(name, param)</code> Adds a parameter to the module. The parameter can be accessed as an attribute using given name.<br><code>load_state_dict(state_dict, strict=True)</code> Copies parameters and buffers from state_dict into this module and its descendants. If strict is True, then the keys of state_dict must exactly match the keys returned by this module’s state_dict() function.<br><code>state_dict(destination=None, prefix=&#39;&#39;, keep_vars=False)</code> Returns a <strong>dictionary</strong> containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</p>
<p><code>children()</code> Returns an <strong>iterator</strong> over immediate children modules.<br><code>modules()</code> Returns an <strong>iterator</strong> over all modules in the network. Duplicate modules are returned only once.<br><code>named_children()</code> Returns an <strong>iterator</strong> over immediate children modules, yielding both the name of the module as well as the module itself.<br><code>named_modules(memo=None, prefix=&#39;&#39;)</code> Returns an <strong>iterator</strong> over all modules in the network, yielding both the name of the module as well as the module itself. Duplicate modules are returned only once.</p>
<h3 id="Module中的parameters和buffers和state-dict"><a href="#Module中的parameters和buffers和state-dict" class="headerlink" title="Module中的parameters和buffers和state_dict"></a>Module中的parameters和buffers和state_dict</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/89442276">Pytorch模型中的parameter与buffer</a><br><em>parameter</em> 反向传播需要被optimizer更新, 通过<code>model.parameters()</code>返回；<em>buffer</em> 反向传播不需要被optimizer更新，通过<code>model.buffers()</code>返回。<br>创建parameter：<br>(1) 直接将模型的成员变量(self.xxx)通过<code>nn.Parameter()</code>创建，会自动注册到parameters中，并且这样创建的参数会自动保存到state_dict中；<br>(2) 通过<code>nn.Parameter()</code>创建普通Parameter对象，不作为模型的成员变量，然后将Parameter对象通过<code>register_parameter()</code>进行注册，注册后的参数也会自动保存到state_dict中。<br>创建buffer：<br>需要创建tensor，然后将tensor通过<code>register_buffer()</code>进行注册，注册完后参数也会自动保存到state_dict中去。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        buffer = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;my_buffer&#x27;</span>, buffer)</span><br><span class="line">        self.param1 = nn.Parameter(torch.randn(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        param = nn.Parameter(torch.randn(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        self.register_parameter(<span class="string">&quot;param2&quot;</span>, param)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = MyModel()</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="built_in">print</span>(param)  <span class="comment"># &lt;class &#x27;torch.nn.parameter.Parameter&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> tup <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="built_in">print</span>(tup)  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> buffer <span class="keyword">in</span> model.buffers():</span><br><span class="line">        <span class="built_in">print</span>(buffer)  <span class="comment"># &lt;class &#x27;torch.Tensor&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> tup <span class="keyword">in</span> model.named_buffers():</span><br><span class="line">        <span class="built_in">print</span>(tup)  <span class="comment"># &lt;class &#x27;tuple&#x27;&gt;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(model.state_dict())  <span class="comment"># &lt;class &#x27;collections.OrderedDict&#x27;&gt;</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-<span class="number">0.9896</span>, -<span class="number">1.9619</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-<span class="number">1.6176</span>],</span><br><span class="line">        [-<span class="number">0.1562</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">----------------</span><br><span class="line">(<span class="string">&#x27;param1&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([[-<span class="number">0.9896</span>, -<span class="number">1.9619</span>]], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;param2&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([[-<span class="number">1.6176</span>],</span><br><span class="line">        [-<span class="number">0.1562</span>]], requires_grad=<span class="literal">True</span>))</span><br><span class="line">----------------</span><br><span class="line">tensor([[-<span class="number">1.3587</span>, -<span class="number">0.4721</span>, -<span class="number">0.7553</span>],</span><br><span class="line">        [ <span class="number">0.5493</span>,  <span class="number">0.8100</span>,  <span class="number">1.4646</span>]])</span><br><span class="line">----------------</span><br><span class="line">(<span class="string">&#x27;my_buffer&#x27;</span>, tensor([[-<span class="number">1.3587</span>, -<span class="number">0.4721</span>, -<span class="number">0.7553</span>],</span><br><span class="line">        [ <span class="number">0.5493</span>,  <span class="number">0.8100</span>,  <span class="number">1.4646</span>]]))</span><br><span class="line">----------------</span><br><span class="line">OrderedDict([(<span class="string">&#x27;param1&#x27;</span>, tensor([[-<span class="number">0.9896</span>, -<span class="number">1.9619</span>]])), (<span class="string">&#x27;param2&#x27;</span>, tensor([[-<span class="number">1.6176</span>],</span><br><span class="line">        [-<span class="number">0.1562</span>]])), (<span class="string">&#x27;my_buffer&#x27;</span>, tensor([[-<span class="number">1.3587</span>, -<span class="number">0.4721</span>, -<span class="number">0.7553</span>],</span><br><span class="line">        [ <span class="number">0.5493</span>,  <span class="number">0.8100</span>,  <span class="number">1.4646</span>]]))])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存和加载</span></span><br><span class="line">torch.save(model.state_dict(), PATH)</span><br><span class="line">model.load_state_dict(torch.load(PATH)</span><br></pre></td></tr></table></figure>
<h3 id="Module中的children和modules"><a href="#Module中的children和modules" class="headerlink" title="Module中的children和modules"></a>Module中的children和modules</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModel, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.lin2 = nn.Sequential(nn.Linear(<span class="number">1</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">        lin = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.lin3 = nn.Sequential(lin, lin)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    model = MyModel()</span><br><span class="line">    <span class="keyword">for</span> tup <span class="keyword">in</span> model.named_children():</span><br><span class="line">        <span class="built_in">print</span>(tup)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> tup <span class="keyword">in</span> model.named_modules():</span><br><span class="line">        <span class="built_in">print</span>(tup)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="keyword">for</span> tup <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">        <span class="built_in">print</span>(tup)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;----------------&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(model.state_dict())</span><br><span class="line"><span class="comment">##</span></span><br><span class="line">(<span class="string">&#x27;lin1&#x27;</span>, Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2&#x27;</span>, Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">))</span><br><span class="line">(<span class="string">&#x27;lin3&#x27;</span>, Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">))</span><br><span class="line">----------------</span><br><span class="line">(<span class="string">&#x27;&#x27;</span>, MyModel(</span><br><span class="line">  (lin1): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (lin2): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">  (lin3): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">))</span><br><span class="line">(<span class="string">&#x27;lin1&#x27;</span>, Linear(in_features=<span class="number">1</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2&#x27;</span>, Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">1</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br><span class="line">))</span><br><span class="line">(<span class="string">&#x27;lin2.0&#x27;</span>, Linear(in_features=<span class="number">1</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2.1&#x27;</span>, Linear(in_features=<span class="number">2</span>, out_features=<span class="number">1</span>, bias=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin3&#x27;</span>, Sequential(</span><br><span class="line">  (<span class="number">0</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (<span class="number">1</span>): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">))</span><br><span class="line">(<span class="string">&#x27;lin3.0&#x27;</span>, Linear(in_features=<span class="number">2</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>))</span><br><span class="line">----------------</span><br><span class="line">(<span class="string">&#x27;lin1.weight&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.2448</span>]], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin1.bias&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.7488</span>], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2.0.weight&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([[<span class="number">0.4151</span>],</span><br><span class="line">        [<span class="number">0.9873</span>]], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2.0.bias&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([-<span class="number">0.0301</span>,  <span class="number">0.3848</span>], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2.1.weight&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([[ <span class="number">0.4902</span>, -<span class="number">0.4937</span>]], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin2.1.bias&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([<span class="number">0.2317</span>], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin3.0.weight&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([[-<span class="number">0.6023</span>, -<span class="number">0.2234</span>],</span><br><span class="line">        [ <span class="number">0.4948</span>,  <span class="number">0.4749</span>]], requires_grad=<span class="literal">True</span>))</span><br><span class="line">(<span class="string">&#x27;lin3.0.bias&#x27;</span>, Parameter containing:</span><br><span class="line">tensor([ <span class="number">0.2648</span>, -<span class="number">0.4021</span>], requires_grad=<span class="literal">True</span>))</span><br><span class="line">----------------</span><br><span class="line">OrderedDict([(<span class="string">&#x27;lin1.weight&#x27;</span>, tensor([[<span class="number">0.2448</span>]])), (<span class="string">&#x27;lin1.bias&#x27;</span>, tensor([-<span class="number">0.7488</span>])), (<span class="string">&#x27;lin2.0.weight&#x27;</span>, tensor([[<span class="number">0.4151</span>],</span><br><span class="line">        [<span class="number">0.9873</span>]])), (<span class="string">&#x27;lin2.0.bias&#x27;</span>, tensor([-<span class="number">0.0301</span>,  <span class="number">0.3848</span>])), (<span class="string">&#x27;lin2.1.weight&#x27;</span>, tensor([[ <span class="number">0.4902</span>, -<span class="number">0.4937</span>]])), (<span class="string">&#x27;lin2.1.bias&#x27;</span>, tensor([<span class="number">0.2317</span>])), (<span class="string">&#x27;lin3.0.weight&#x27;</span>, tensor([[-<span class="number">0.6023</span>, -<span class="number">0.2234</span>],</span><br><span class="line">        [ <span class="number">0.4948</span>,  <span class="number">0.4749</span>]])), (<span class="string">&#x27;lin3.0.bias&#x27;</span>, tensor([ <span class="number">0.2648</span>, -<span class="number">0.4021</span>])), (<span class="string">&#x27;lin3.1.weight&#x27;</span>, tensor([[-<span class="number">0.6023</span>, -<span class="number">0.2234</span>],</span><br><span class="line">        [ <span class="number">0.4948</span>,  <span class="number">0.4749</span>]])), (<span class="string">&#x27;lin3.1.bias&#x27;</span>, tensor([ <span class="number">0.2648</span>, -<span class="number">0.4021</span>]))])</span><br></pre></td></tr></table></figure>
<h2 id="torch-autograd-Function"><a href="#torch-autograd-Function" class="headerlink" title="torch.autograd.Function"></a>torch.autograd.Function</h2><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.3.1/notes/extending.html">EXTENDING PYTORCH</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Exp</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, i</span>):</span><br><span class="line">        result = i.exp()</span><br><span class="line">        ctx.save_for_backward(result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        result, = ctx.saved_tensors</span><br><span class="line">        <span class="keyword">return</span> grad_output * result</span><br><span class="line"><span class="comment"># To make it easier to use these custom ops, we recommend aliasing their apply method.</span></span><br><span class="line">exp = Exp.apply</span><br></pre></td></tr></table></figure>
<p><code>STATIC backward(ctx, *grad_outputs)</code> It must accept a context ctx as the first argument, followed by as many outputs did <code>forward()</code> return, and it should return as many tensors, as there were inputs to <code>forward()</code>. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.<br>The context can be used to retrieve tensors saved during the forward pass. It also has an attribute <code>ctx.needs_input_grad</code> as a tuple of booleans representing whether each input needs gradient. E.g., <code>backward()</code> will have <code>ctx.needs_input_grad[0] = True</code> if the first input to <code>forward()</code> needs gradient computated w.r.t. the output.</p>
<p><code>STATIC forward(ctx, *args, **kwargs)</code> It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass.</p>
<h2 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h2><p>torch.Tensor: class, is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the torch.tensor() constructor:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1.</span>, -<span class="number">1.</span>], [<span class="number">1.</span>, -<span class="number">1.</span>]])</span><br><span class="line">torch.tensor(np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]))</span><br></pre></td></tr></table></figure>
<h3 id="item"><a href="#item" class="headerlink" title="item()"></a>item()</h3><p><code>x.item()</code> Returns the value of this tensor as a standard Python number. This only works for tensors with one element. This operation is not differentiable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>]])</span><br><span class="line">x  <span class="comment"># tensor([[1]])</span></span><br><span class="line">x.item()  <span class="comment"># 1</span></span><br></pre></td></tr></table></figure>
<h3 id="clone-和detach"><a href="#clone-和detach" class="headerlink" title="clone()和detach()"></a>clone()和detach()</h3><p><code>x.clone()</code> Returns a copy of the self tensor. The copy has the same size and data type as self. Unlike <code>copy_()</code>, this function is <strong>recorded in the computation graph</strong>. Gradients propagating to the cloned tensor will propagate to the original tensor. 开辟新的内存。<br><code>x.detach()</code> Returns a new Tensor, detached from the current graph. The result will <strong>never require gradient</strong>. Returned Tensor <strong>shares the same storage</strong> with the original one.<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/guofei_fly/article/details/104486708">【Pytorch】对比clone、detach以及copy_等张量复制操作</a></p>
<h3 id="contiguous"><a href="#contiguous" class="headerlink" title="contiguous()"></a>contiguous()</h3><p>Returns a contiguous tensor containing the same data as self tensor. If self tensor is contiguous, this function returns the self tensor.<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/64551412">PyTorch中的contiguous</a></p>
<h3 id="index-add-dim-index-tensor-→-Tensor"><a href="#index-add-dim-index-tensor-→-Tensor" class="headerlink" title="index_add_(dim, index, tensor) → Tensor"></a>index_add_(dim, index, tensor) → Tensor</h3><p>Accumulate the elements of <code>tensor</code> into the self tensor by adding to the indices in the order given in <code>index</code>.<br>For example, if <code>dim == 0</code> and <code>index[i] == j</code>, then the <code>i</code> th row of tensor is added to the <code>j</code> th row of self.<br><strong>dim</strong>: int, dimension along which to index.<br><strong>index</strong>: LongTensor, indices of tensor to select from.<br><strong>tensor</strong>: Tensor, the tensor containing values to add.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">index = torch.tensor([<span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">x.index_add_(<span class="number">0</span>, index, t)</span><br><span class="line"><span class="comment"># tensor([[  2.,   3.,   4.],</span></span><br><span class="line"><span class="comment">#         [  1.,   1.,   1.],</span></span><br><span class="line"><span class="comment">#         [  8.,   9.,  10.],</span></span><br><span class="line"><span class="comment">#         [  1.,   1.,   1.],</span></span><br><span class="line"><span class="comment">#         [  5.,   6.,   7.]])</span></span><br></pre></td></tr></table></figure>
<h2 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h2><p>Transforms on PIL Image: 输入为PIL Image，输出也为PIL Image。</p>
<p><strong>torchvision.transforms.ToTensor</strong>: Convert a PIL Image or numpy.ndarray to tensor.<br>Converts a <strong>PIL Image</strong> or <strong>numpy.ndarray (H x W x C) in the range [0, 255]</strong> to a <strong>torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]</strong> if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype &#x3D; np.uint8.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PIL.Image读的图片为 W*H，通道为 RGB</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">im = Image.<span class="built_in">open</span>(fpath).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(im.size)  <span class="comment"># (w, h)</span></span><br><span class="line"><span class="built_in">print</span>(im.mode)  <span class="comment"># &#x27;RGB&#x27;</span></span><br><span class="line">a = np.asarray(im)</span><br><span class="line"><span class="built_in">print</span>(a.shape)  <span class="comment"># (h, w, c)</span></span><br><span class="line">im = Image.fromarray(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># opencv读的图片为 H*W，通道为 BGR</span></span><br><span class="line">im = cv2.imread(fpath, cv2.IMREAD_COLOR)</span><br><span class="line"><span class="built_in">print</span>(im.shape)  <span class="comment"># (h, w, c)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># a的通道为 RGB，im的通道为 BGR</span></span><br></pre></td></tr></table></figure>
<h2 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions"></a>Loss functions</h2><h3 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h3><p>CLASS: <code>torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code><br>FUNCTION: <code>torch.nn.functional.nll_loss(input, target, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code><br>The negative log likelihood loss. It is useful to train a classification problem with C classes.</p>
<p><strong>size_average</strong>: Deprecated.<br><strong>reduce</strong>: Deprecated.<br><strong>weight</strong>: a manual rescaling weight given to each class. If given, it has to be a Tensor of size $C$. Otherwise, it is treated as if having all ones. This is particularly useful when you have an unbalanced training set.<br><strong>ignore_index</strong>: Specifies a target value that is ignored and does not contribute to the input gradient. When <code>reduction=&#39;mean&#39;</code>, the loss is averaged over non-ignored targets.<br><strong>reduction</strong>: Specifies the reduction to apply to the output: <code>&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;</code>.</p>
<p><em><strong>input</strong></em>: log-probabilities of each class, Tensor of size either $(N,C)$ or $(N,C,d_1,d_2,\ldots,d_K)$ with $K\geq1$ for the K-dimensional case.<br>Obtaining log-probabilities in a neural network is easily achieved by adding a <em>LogSoftmax</em> layer in the last layer of your network. You may use <em>CrossEntropyLoss</em> instead, if you prefer not to add an extra layer.<br><em><strong>target</strong></em>: class index in the range $[0,C-1]$; if <em>ignore_index</em> is specified, this loss also accepts this class index (this index may not necessarily be in the class range). Tensor of size either $(N)$ or $(N,d_1,d_2,\ldots,d_K)$ with $K\geq1$ for the K-dimensional case.<br><em><strong>Output</strong></em>: scalar. If reduction is ‘none’, then the same size as the target: $(N)$ or $(N,d_1,d_2,\ldots,d_K)$ with $K\geq1$ for the K-dimensional case.</p>
<p><code>reduction=&#39;none&#39;</code>, loss:<br><img src="/download/20.png"><br><code>reduction=&#39;mean&#39;</code>or <code>reduction=&#39;sum&#39;</code>, loss:<br><img src="/download/21.png"><br>In the K-dimensional case, it computes NLL loss per-pixel.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="built_in">input</span> = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[-1.1773, -3.1962, -2.5500, -0.9069, -1.7771],</span></span><br><span class="line"><span class="comment">#         [-1.0217, -1.2633, -1.9184, -2.0835, -2.4538],</span></span><br><span class="line"><span class="comment">#         [-3.0267, -2.3012, -2.4655, -0.6980, -1.3137]],</span></span><br><span class="line"><span class="comment">#        grad_fn=&lt;LogSoftmaxBackward&gt;)</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># tensor([3.1962, 1.0217, 1.3137], grad_fn=&lt;NllLossBackward&gt;)</span></span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target)</span><br><span class="line"><span class="comment"># tensor(1.8439, grad_fn=&lt;NllLossBackward&gt;)</span></span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target, weight=torch.tensor([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">5.</span>]), reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># tensor([6.3925, 1.0217, 6.5687], grad_fn=&lt;NllLossBackward&gt;)</span></span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target, ignore_index=<span class="number">4</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># tensor([3.1962, 1.0217, 0.0000], grad_fn=&lt;NllLossBackward&gt;)</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>])</span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target, ignore_index=-<span class="number">1</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># tensor([3.1962, 0.0000, 0.0000], grad_fn=&lt;NllLossBackward&gt;)</span></span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target, weight=torch.tensor([<span class="number">1.</span>,<span class="number">2.</span>,<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">5.</span>]), ignore_index=-<span class="number">1</span>, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line"><span class="comment"># tensor([6.3925, 0.0000, 0.0000], grad_fn=&lt;NllLossBackward&gt;)</span></span><br><span class="line">output = F.nll_loss(F.log_softmax(<span class="built_in">input</span>, dim=<span class="number">1</span>), target, ignore_index=-<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tensor(3.1962, grad_fn=&lt;NllLossBackward&gt;)</span></span><br></pre></td></tr></table></figure>
<h3 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h3><p>CLASS: <code>torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=&#39;mean&#39;)</code><br>This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.<br>The <em>input</em> is expected to contain raw, unnormalized scores for each class.<br><img src="/download/25.png"><br><img src="/download/26.png"></p>
<h3 id="BCEWithLogitsLoss"><a href="#BCEWithLogitsLoss" class="headerlink" title="BCEWithLogitsLoss"></a>BCEWithLogitsLoss</h3><p>CLASS: <code>torch.nn.BCEWithLogitsLoss(weight=None, size_average=None, reduce=None, reduction=&#39;mean&#39;, pos_weight=None)</code><br>This loss combines a <em>Sigmoid</em> layer and the <em>BCELoss</em> in one single class.</p>
<p><strong>size_average</strong>: Deprecated.<br><strong>reduce</strong>: Deprecated.<br><strong>weight</strong>: a manual rescaling weight given to the loss of each batch element. If given, has to be a Tensor of size nbatch.<br><strong>reduction</strong>: Specifies the reduction to apply to the output: <code>&#39;none&#39; | &#39;mean&#39; | &#39;sum&#39;</code>.<br><strong>pos_weight</strong>: a weight of positive examples. Must be a vector with length equal to the number of classes.</p>
<p><em><strong>Input</strong></em>: (N, #).<br><em><strong>target</strong></em>: (N, #), numbers between 0 and 1.<br><em><strong>Output</strong></em>: scalar. If reduction is ‘none’, then (N, #), same shape as input.</p>
<p><code>reduction=&#39;none&#39;</code>, loss:<br><img src="/download/22.png"><br><code>reduction=&#39;mean&#39;</code>or <code>reduction=&#39;sum&#39;</code>, loss:<br><img src="/download/23.png"><br>It’s possible to trade off recall and precision by adding weights to positive examples. multi-label classification:<br><img src="/download/24.png"></p>
<h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><p><code>torch.nn.functional.normalize(input, p=2, dim=1, eps=1e-12, out=None)</code><br>Performs $L_p$ normalization of inputs over specified dimension.<br>$$<br>v&#x3D;\frac{v}{\max(||v||_p, \epsilon)}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">a = torch.tensor([[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">a = F.normalize(a)</span><br><span class="line"><span class="comment"># tensor([[0.5774, 0.5774, 0.5774],</span></span><br><span class="line"><span class="comment">#         [0.3333, 0.6667, 0.6667]])</span></span><br></pre></td></tr></table></figure>
<p><code>x.norm(p=&#39;fro&#39;, dim=None, keepdim=False, dtype=None)</code><br><code>torch.norm(input, p=&#39;fro&#39;, dim=None, keepdim=False, out=None, dtype=None)</code><br>Returns the matrix norm or vector norm of a given tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">a.norm()  <span class="comment"># tensor(3.)</span></span><br><span class="line">a / a.norm()  <span class="comment"># tensor([0.3333, 0.6667, 0.6667])</span></span><br></pre></td></tr></table></figure>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/11/05/re-ID%E7%9B%B8%E5%85%B3/" rel="prev" title="re-ID 相关">
      <i class="fa fa-chevron-left"></i> re-ID 相关
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/11/20/Linux%E7%9B%B8%E5%85%B3/" rel="next" title="Linux 相关">
      Linux 相关 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-nn-Parameter%E5%92%8Ctorch-nn-Module"><span class="nav-number">1.</span> <span class="nav-text">torch.nn.Parameter和torch.nn.Module</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parameter%E5%92%8CModule%E7%9A%84%E5%AE%98%E6%96%B9%E8%AF%B4%E6%98%8E"><span class="nav-number">1.1.</span> <span class="nav-text">Parameter和Module的官方说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Module%E4%B8%AD%E7%9A%84parameters%E5%92%8Cbuffers%E5%92%8Cstate-dict"><span class="nav-number">1.2.</span> <span class="nav-text">Module中的parameters和buffers和state_dict</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Module%E4%B8%AD%E7%9A%84children%E5%92%8Cmodules"><span class="nav-number">1.3.</span> <span class="nav-text">Module中的children和modules</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-autograd-Function"><span class="nav-number">2.</span> <span class="nav-text">torch.autograd.Function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-Tensor"><span class="nav-number">3.</span> <span class="nav-text">torch.Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#item"><span class="nav-number">3.1.</span> <span class="nav-text">item()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#clone-%E5%92%8Cdetach"><span class="nav-number">3.2.</span> <span class="nav-text">clone()和detach()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#contiguous"><span class="nav-number">3.3.</span> <span class="nav-text">contiguous()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#index-add-dim-index-tensor-%E2%86%92-Tensor"><span class="nav-number">3.4.</span> <span class="nav-text">index_add_(dim, index, tensor) → Tensor</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torchvision-transforms"><span class="nav-number">4.</span> <span class="nav-text">torchvision.transforms</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-functions"><span class="nav-number">5.</span> <span class="nav-text">Loss functions</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#NLLLoss"><span class="nav-number">5.1.</span> <span class="nav-text">NLLLoss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CrossEntropyLoss"><span class="nav-number">5.2.</span> <span class="nav-text">CrossEntropyLoss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BCEWithLogitsLoss"><span class="nav-number">5.3.</span> <span class="nav-text">BCEWithLogitsLoss</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-nn-functional"><span class="nav-number">6.</span> <span class="nav-text">torch.nn.functional</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Xiaowen"
      src="/img/avatar.jpg">
  <p class="site-author-name" itemprop="name">Xiaowen</p>
  <div class="site-description" itemprop="description">Learn something</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:xiaowen_shao@163.com" title="E-Mail → mailto:xiaowen_shao@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://blog.csdn.net/sxwzuoyi" title="CSDN → https:&#x2F;&#x2F;blog.csdn.net&#x2F;sxwzuoyi" rel="noopener" target="_blank"><i class="fa fa-fw fa-link"></i>CSDN</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://blog.cyyan.cn/" title="http:&#x2F;&#x2F;blog.cyyan.cn&#x2F;" rel="noopener" target="_blank">Chaoyang</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xiaowen</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
